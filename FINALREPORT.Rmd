---
title: "HW6"
author: "Gabriel Alwan, Neo Kok, Kevin Lu, Liz Orraca"
output:
  pdf_document: 
    latex_engine: xelatex
  html_document: default
---



**METHODS:**

```{r}
#load in necessary packages
library(tidyverse)
library(BradleyTerry2)
library(dplyr)
library(lme4)
library(brms)
library(rstanarm)
library(knitr)
```


```{r}
#Starts the clock to test the efficiency
start_time <- Sys.time()

```


```{r}

#data cleaning

ncaa_data = read_csv("ncaa_data.csv") %>% filter(season > 2000, current_division == "D1")
ncaa_teams = read_csv("ncaa_teams.csv")
seeds = read_csv("team_seeds_2000_2016.csv") %>% select(season, school_name, seed)
```


```{r}

#function below modifies the data for each year and then fits a bradley terry model (using the function glm )
process_model = function(ncaa, teams, seeds, season){
  ncaa = ncaa %>% select(season, team_code, points_game, win, opp_code, opp_points_game) %>% filter(season == season)
  teams = teams %>% select(school_ncaa, team_code = code_ncaa, conf_name) 
  
  data = left_join(ncaa, select(teams, home_team = school_ncaa, team_code, home_conf = conf_name), by = c("team_code")) 
  data = left_join(data, select(teams, away_team = school_ncaa, opp_code = team_code, away_conf = conf_name), by = c("opp_code"))
  
  #Some teams dropped from d1 to d2 or lower so dropped them
  data = data %>% na.omit()
  
  data = data %>% mutate(point_diff = points_game - opp_points_game, win = ifelse(win, 1, 0)) %>%
    select(season, home_team, away_team, result = win, home_conf, away_conf, point_diff)
  
  
  home_counts <- table(data$home_team)
  away_counts <- table(data$away_team)
  
  # Combine counts for home and away games
  total_counts <- home_counts + away_counts
  
  # Step 2: Filter teams with 10 or more games
  valid_teams <- names(total_counts[total_counts > 10])
  
  # Step 3: Filter the dataset to include only valid teams
  filtered_data <- subset(data, home_team %in% valid_teams & away_team %in% valid_teams)
  # Step 1: Reshape the data for both teams
  data_long <- filtered_data %>%
    pivot_longer(
      cols = c(home_team, away_team), 
      names_to = "location", 
      values_to = "team"
    ) %>%
    mutate(
      home = ifelse(location == "home_team", 1, 0),
      result = ifelse(location == "home_team", result, 1 - result)
    )
  
  print(paste0("Fitting model ", season))
  # Step 2: Fit the logistic regression model
  bt_model <- glm(
    result ~ home + as.factor(team) + as.factor(home_conf) + as.factor(away_conf) - 1,
    family = binomial(link = "logit"),
    data = data_long
  )
  
  team_coefficients <- coef(bt_model)[grep("^as.factor\\(team\\)", names(coef(bt_model)))]
  names(team_coefficients) <- gsub("as.factor\\(team\\)", "", names(team_coefficients))
  ranked_teams <- sort(team_coefficients, decreasing = TRUE)
  
  ranked_teams <- data.frame(
    Team = names(ranked_teams),
    Strength = ranked_teams
  )
  
  
  top_64 = ranked_teams[1:64,] %>% mutate(season = season)
  top_64$school_name = rownames(top_64)
  top_64$bt_seed <- rep(1:16, each = 4)
  ranking = left_join(top_64, seeds, by = c("school_name", "season"))
  
  return(ranking)
}
```


```{r}

#calls the function above and then combines all of the predictions across all years into one data set
combined_results = process_model(ncaa_data, ncaa_teams, seeds, 2000)
for(season in c(2001:2016)){
  combined_results = rbind(combined_results, process_model(ncaa_data, ncaa_teams, seeds, season))
  
}

combined_results %>% na.omit() %>% group_by(season) %>% summarise(num = n(), na = 64 - num, same = sum(bt_seed == seed), diff = sum((bt_seed - as.integer(seed)), na.rm = T))


write_csv(combined_results, "ranking_comparison.csv")


```



```{r}

#final time
end_time <- Sys.time()
total_time <- end_time - start_time
print(total_time)

```


**RESULTS**

Read in data:
```{r}

x = read_csv("ranking_comparison.csv")
differences = read_csv("differences.csv")
point_differential = read_csv("points_diff_p_values.csv")
deviance = read_csv("deviances.csv")


```


Most accurate year:
```{r}
best_year = c()
count_max = 0

for(i in 2000:2016){
  
  df_year_i = combined_results%>%filter(season == i)
  num_correctly_identified = nrow(df_year_i) - sum(is.na(df_year_i$seed))
  if(num_correctly_identified > count_max){
    count_max = num_correctly_identified
    best_year = c(i)
  }
  if(num_correctly_identified == count_max){
     best_year = appeand(best_year, i) 
    
  }
  
}

print(best_year)

```

Least accurate year:
```{r}

worst_year = c()
count_max = 99999

for(i in 2000:2016){
  
  df_year_i = combined_results%>%filter(season == i)
  num_correctly_identified = nrow(df_year_i) - sum(is.na(df_year_i$seed))
  if(num_correctly_identified < count_max){
    count_max = num_correctly_identified
    worst_year = c(i)
  }
  if(num_correctly_identified == count_max){
     worst_year = appeand(worst_year, i) 
    
  }
  
}

print(worst_year)


```

After running our methods, we produce some other data sets that will help us interpret the results. 
Before getting into what our predictions actually were, we want to produce a metric that expands on how well our model actually fits the data. 
A common way to interpret this in the context of a Bradley-Terry model (a specific type of generalized linear model as explained above) is to measure the deviance. In a general context, the deviance compares the likelihood of our fitted model to another complete model. 
This complete model essentially has no residual errors, which is a complete over fit of a model. 
In our case our likelihoods were calculated on a binomial distribution, which was a key assumption for the data. 

In purely mathematical terms,

$deviance = -2 \cdot [ln(Likelihood(model_{fitted})) - ln(Likelihood(model_{complete}))]$.

For information https://en.wikipedia.org/wiki/Deviance_(statistics). 

A low deviance informs us that the model has fit the data well. 
However, a raw deviance statistic doesn't indicate too much without context, as we have to consider things such as sample size. 
Our deviance were indeed large, but we are dealing with big data. Instead of looking at a standardized form of deviance (taking into account sample size), we want to modify to get another modification to this metric called `Deviance reduction`. 
In order to calculate this, we need two different types of deviance for each year (2000-2016): null deviance and residual deviance.
The null deviance calculates the fit of the model without any predictors. The residual deviance includes our desired predictors (home, point differential, school conference). 
Thus, the difference between the two gives us the reduction.
In order to get the `Deviance explained` by the model (in terms of a percentage), we divide the reduction by the null deviance. Mathematically, 

deviance explained = [(null deviance) - (residual deviance)]/ (null deviance ) * 100 = (deviance reduction)/ (null deviance ) * 100.


Below we plot the deviance explained by the model for each respective year.

```{r}
deviance$deviance_explained = (deviance$null_deviance - deviance$model_deviance)/deviance$null_deviance *100

ggplot(deviance, aes(x = factor(season), y = deviance_explained)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black")+ 
  geom_text(aes(label = round(deviance_explained, 2)), vjust = -0.5, color = "black", size = 3) + 
  labs(title = "Deviance Explained by Year", x = "Season", y = "Deviance Explained (%)"
  )+
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 60, hjust = 1))


```

We notice that the percentages for each season hover around 20%. This can indicate a moderate fit of the model to the data. There are many good reasons for this inconsistency which can be adapted for future analyses. 
First, as we will discuss below, there is an inconsistency with our results based off of automatic qualification criteria. 
Additionally, big data, especially in the context of sports, can be extremely noisy. 
A low deviance percentage may indicate an under-fit and a need for more covariates in the model. 
This makes sense, as there are so many different factors that can influence a game outside of the variables that we included in the model. 
These factors could include injuries, a winning streak (momentum), or other non-quantifiable factors (such as a teams playbook/style of play). 
In the future, we would explore and try to find a more comprehensive dataset that includes these factors. 


Now, we want to show what our model actually predicted compared to what the committee actually picked. 
Below we produce a graph of our results. 
The graph includes all 64 predictions each year, 
indicates which teams weren't actually in the tournament, which teams we correctly seeded, 
and which teams we correctly included but didn't properly seed. 

```{r}

data_long <- differences %>%
  mutate(
    correct = num, misseeds = correct - same, incorrect = na  
  ) %>%
  select(season, same, misseeds, incorrect) %>%
  pivot_longer(cols = c(same, misseeds, incorrect), 
               names_to = "PredictionType", 
               values_to = "Count") %>%
  mutate(PredictionType = factor(PredictionType, levels = c("same", "misseeds", "incorrect")))


total_predictions <- data_long %>%
  group_by(season) %>%
  summarise(Total = sum(Count))


ggplot(data_long, aes(x = factor(season), y = Count)) +
  geom_bar(stat = "identity", position = "stack", aes(fill = PredictionType)) +
  labs(
    title = "Prediction Accuracy for College Basketball Tournament (2000-2016)",
    x = "Season",
    y = "Number of Predictions",
    fill = "Prediction Type"
  ) +
  theme_minimal() +
  scale_fill_manual(values = c("lightgreen", "orange", "red")) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  
  theme(plot.title = element_text(hjust = 0.5)) +  
  
  geom_text(data = total_predictions, 
            aes(x = factor(season), y = Total + 2, label = "64"), 
            color = "black", size = 4, vjust = 0)  

  
```

The results of this graphs are consistent with the deviance metrics produced above. 
However, in the context of the automatic qualifier problem, we aren't entirely surprised/discourages about the results.
Unfortunately, our model can't account for automatic qualification as we don't have the data to back it up. 
From 2000 to 2016, there were roughly 32 conferences each year. In terms of player abilities, 
the competition across conferences is not even remotely even. Anecdotally speaking, teams from big time conferences (BIG 10, ACC, SEC, BIG EAST, etc.) are way more talented than that from smaller conferences. 
These big time conferences attract better recruits, have access to better facilities, better coaches and atheltic training. Thus, the tournament year in and year out is forced to include sub par teams in order to be fair to smaller schools. 
So, the fact that we would just over half the teams correct indicates that our model actually is correctly identifying the top teams. 
All of the incorrect teams are schools from big time conferences who were wrongfully not considered a top 64 team. 
Actually, these results can be served as a recommendation 
to the selection committee to get rid of automatic qualifying in order to get the actual best teams in the tournament, based off of our quantitative criteria. 
We show the discrepancy how the best teams are from a select few conferences, 
by showing the the number of teams the conference produced by year from our predictions. 

Below is a graph that shows this year by year. 


```{r}
ncaa_teams = read_csv("ncaa_teams.csv")
ncaa_teams$Team = ncaa_teams$school_ncaa

merged_data <- merge(ncaa_teams, combined_results, by = "Team")

conference_predictions <- merged_data %>%
  group_by(season, conf_alias) %>%
  summarize(predicted_teams = n(), .groups = 'drop')

ggplot(conference_predictions, aes(x = season, y = predicted_teams, fill = conf_alias)) +
  geom_bar(stat = "identity", position = "stack") +
  labs(title = "Number of Predicted Teams from Each Conference by Year",
       x = "Season",
       y = "Number of Predicted Teams",
       fill = "Conference") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 




```



We notice that our predictions are dominated by teams from all of the top conferences: ACC, BIG10, BIG12, BIG EAST, PAC 12, SEC. 
The only two outlier conferences were the WCC and the AAC. 
These teams were Gonzaga (WCC), UConn (AAC) and Memphis (AAC) which are some of the three most historically dominate programs in the nation. 
Our predictions exuded the other 25ish other conferences which include automatic qualifiers. Thus, it makes sense that we had about under 30 incorrect predictions each year (from the first graph). 


We now want to talk about the accuracy of our seeding for the model. We want to find the average percentage difference in seeding for teams we correctly predicted in the tournament, but had an incorrect seeding for. We produce this graph below.
```{r}


differences <- differences %>%
  mutate(diff_percent = (diff / num) * 100)  # Calculate diff as a percentage of num


ggplot(differences, aes(x = factor(season))) +
  geom_bar(aes(y = diff_percent, fill = num), stat = "identity", width = 0.7) +
  labs(
    title = "Seeding Difference as Percentage of Correct Predictions (2000-2016)",
    x = "Season",
    y = "Seeding Difference (%)",
    fill = "Correct Predictions (num)"
  ) +
  scale_fill_gradient(low = "yellow", high = "green") + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  theme(plot.title = element_text(hjust = 0.5)) 


```
We notice a sort of inverse relationship. For years that we had less correct predictions in the tournament, we notice that our seedings were seemingly more accurate (e.g. 2004, 2014). 
On the other hand, years that we got a lot of teams correct had more incorrect seedings (e.g. 2001, 2010). 
This is an interesting wrinkle in our results. 
Perhaps, sacrificing accuracy for getting the right teams in comes at the expense of appropriately getting the seeding correct, which could be a limitation of the Bradley-Terry approach. 
Also, note that these seedings may be vastly different as per the discussion above. 
Historically speaking, teams from weaker conferences are awarded seeds that are lower (16, 15, 14, ...) whereas at large 
teams (teams that qualified from committee selection) from stronger conferences are higher seeds (starting from 12, 11, 10...).
Again, the automatic bid criteria is producing seemingly inaccurate conclusions about the appropriateness of our model. 


Finally, we want to discuss one covaraite that we thought would be significant in the model: point differential. 
This is basically the number of points that a teams wins or loses by. 
We would presume that teams that 
consistently produces large point differentials in wins would be more likely to be included in the tournament, as that would indicate the team is very strong. 
However, looking at the table below for each yearly model, we can't come to that conclusion.

```{r}

print(point_differential)

```

However, after seeing these results, this is entirely predictable. As we discussed above, teams from stronger conferences are heavily more favored by our model. 
If a team is playing in a strong conference, you're going to consistently be playing against elite competition, 
thus leading to more loses and close wins. 
Additionally, we included a conference variable in our model, thus there may be some redundancy and multicollinearity between the two variables leading to insignificant results. 
Teams in worse conferences may experience more blowouts as there may be better top end competition. 
For example, take Gonzaga who plays in the WCC. 
The talent gap between them and the worst team is presumably much greater than that of the best and worst BIG 10 teams, for example. 
Thus, in these other conferences, there may be more blowouts. 


